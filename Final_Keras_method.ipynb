{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D, Convolution2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD,RMSprop,adam,Adam,Adagrad\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import theano\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib\n",
    "import cv2\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from tqdm import tqdm\n",
    "import glob as glob\n",
    "import imutils\n",
    "\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p1 = \"C:/Users/Harsh Pande/Desktop/Final Dataset\"\n",
    "p1 = \"C:/Users/Harsh Pande/Desktop/Final & Flipped Dataset/\"\n",
    "#p2 = \"C:/Users/prana/Desktop/DataScience/processed\"\n",
    "img_rows = 50\n",
    "img_cols = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imglist = os.listdir(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17400\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(imglist)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files = glob.glob (\"C:/Users/Harsh Pande/Desktop/Final Dataset/*.jpg\")\\ni = 0\\nfor myFile in files:\\n    image = cv2.imread (myFile)\\n    mirror_img = np.fliplr(image)\\n    #rotated = imutils.rotate(image, 270)\\n    path = \\'C:/Users/prana/Desktop/DataScience/flipped\\'\\n    cv2.imwrite(os.path.join(path , \"img\"+str(i)+\".jpg\"), mirror_img)\\n    i = i + 1\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''files = glob.glob (\"C:/Users/Harsh Pande/Desktop/Final Dataset/*.jpg\")\n",
    "i = 0\n",
    "for myFile in files:\n",
    "    image = cv2.imread (myFile)\n",
    "    mirror_img = np.fliplr(image)\n",
    "    #rotated = imutils.rotate(image, 270)\n",
    "    path = 'C:/Users/prana/Desktop/DataScience/flipped'\n",
    "    cv2.imwrite(os.path.join(path , \"img\"+str(i)+\".jpg\"), mirror_img)\n",
    "    i = i + 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17400, 50, 50, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "X_data = []\n",
    "files = glob.glob (\"C:/Users/Harsh Pande/Desktop/Final & Flipped Dataset/*.jpg\")\n",
    "for myFile in files:\n",
    "    #print(myFile)\n",
    "    image = cv2.imread (myFile)\n",
    "    resized_image = cv2.resize(image, (img_rows,img_cols)) \n",
    "    X_data.append (resized_image)\n",
    "\n",
    "final = np.array(X_data)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17400, 2500, 3)\n",
      "(7500, 17400)\n",
      "(17400, 7500)\n"
     ]
    }
   ],
   "source": [
    "pixel_lists = final.reshape(final.shape[:-3] + (-1, 3))\n",
    "print(pixel_lists.shape)  \n",
    "\n",
    "new1 = pixel_lists.reshape((pixel_lists.shape[1]*pixel_lists.shape[2]),pixel_lists.shape[0])\n",
    "print(new1.shape)\n",
    "new2 = new1.transpose()\n",
    "print(new2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "label = np.ones((n_samples,),dtype = int)\n",
    "label[0:5799] = 0\n",
    "label[5800:11599] = 1\n",
    "label[11600:17399] = 2\n",
    "print(label[2799])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, Label =shuffle(new2,label,random_state=2)\n",
    "train_data = [data, Label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17400, 7500)\n",
      "(17400,)\n",
      "[255  20  58 255 103  69 255  36 120   0 121 222 255 189  42 177 252  28\n",
      "  94 255 118   1 255 122 185 255 123  45   0 154  95 255  49  68 255  59\n",
      "  61 254 182  52 118 251 150 193 255 174  38 255  84  43 255  73 128 255\n",
      " 121  82 255 162  35 255  62  60 253  43 147 189 253  96  42 255  58 170\n",
      " 255  62  29 255  24  86 255  54  80 255 108  38 255  24  79 255  56  75\n",
      " 255 253 149 199 254  77  99 255  85  73   0  49 155   0  69  73   0  72\n",
      " 137   0 117 139 253  13 129  71 255 123  44 168 111  44   0 111  80   0\n",
      "  60  72   0 127  73   0  69  56 255  42 110 254  42 135 250 253 122  51\n",
      "   0  96 192   0  98  92   0 211 107   0  11 175   0 144 112   0  30  95\n",
      " 255 219 104 237 255 178  52   0  63 163   0 166  95   0 124  87   0  82\n",
      " 182   0 102  55   0  43  46 254 115  99  75 253  95  16 255  89 159   0\n",
      "  70 132   0  91 129   0 168 148   0  73 190   0  31 190 255  14 117 181\n",
      " 255  85  30 255 163  84   0 154 157   0 150 197   0 170 105   0  82  24\n",
      "   0  94  94   0 253 186  64 255 138 136   0 111  23   0 139  19   0 175\n",
      "  74   0  88 141 255  74 115   0  34 194 255  76 106 243 255 155 152   0\n",
      " 144  53   0 119  89   0 143 171   0  67 103   0  57 156   0  65 106 254\n",
      " 254 169 242 255  76 113   0 170 127   0  47  51]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].shape)\n",
    "print(train_data[1].shape)\n",
    "print(train_data[0][2][-300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "batch_size = 256\n",
    "n_epochs = 10\n",
    "n_classes = 3\n",
    "img_channel = 3\n",
    "n_filters = 128\n",
    "n_filters2 = 64\n",
    "n_filters3 = 32\n",
    "n_pool = 2\n",
    "n_conv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "(x , y) = (train_data[0],train_data[1])\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.45,random_state=33)\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],3,img_rows,img_cols)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0],3,img_rows,img_cols)\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "\n",
    "x_train /= 255#standardization\n",
    "x_test /= 255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train,n_classes)\n",
    "y_test = np_utils.to_categorical(y_test,n_classes)\n",
    "\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes1 ===> n_filters, activation,dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(3, 50, 50..., padding=\"valid\")`\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "C:\\Users\\Harsh Pande\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 48, 48)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 48, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 24, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 22, 22)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 22, 22)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 11, 11)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 9, 9)          9248      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 9, 9)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 4, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 2, 2)          9248      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 2, 2)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 1, 1)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 28,739\n",
      "Trainable params: 28,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model  = Sequential()\n",
    "'''\n",
    "K.set_image_dim_ordering('th')\n",
    "model.add(Convolution2D(32,(3,3), input_shape=(3,img_rows,img_cols)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes))\n",
    "\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "model.add(Convolution2D(n_filters3,n_conv,n_conv,border_mode='valid',input_shape=(3,img_rows,img_cols)))#n_filters should be in power of 2\n",
    "convout1 = Activation('relu')\n",
    "model.add(convout1)\n",
    "model.add(MaxPooling2D(pool_size=(n_pool,n_pool),dim_ordering=\"th\"))\n",
    "#model.add(Dropout(0.2))#0.119\n",
    "\n",
    "model.add(Convolution2D(n_filters3,n_conv,n_conv))\n",
    "convout2 = Activation('relu')#(0,x) if x is neg it will be 0 else whatever is the value of x\n",
    "model.add(convout2)\n",
    "model.add(MaxPooling2D(pool_size=(n_pool,n_pool),dim_ordering=\"th\"))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(n_filters3,n_conv,n_conv))\n",
    "convout2 = Activation('relu')\n",
    "model.add(convout2)\n",
    "model.add(MaxPooling2D(pool_size=(n_pool,n_pool),dim_ordering=\"th\"))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(n_filters3,n_conv,n_conv))\n",
    "convout2 = Activation('relu')\n",
    "model.add(convout2)\n",
    "model.add(MaxPooling2D(pool_size=(n_pool,n_pool),dim_ordering=\"th\"))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(32))   ###Changes\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Dense(n_classes))  ###Changes\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dropout(0.5))##changed position\n",
    "\n",
    "model.add(Activation('softmax'))#e to power of x/summation of e to the power of x(basically gives us the probability)\n",
    "\n",
    "\n",
    "#keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer ,metrics=['accuracy'])\n",
    "#keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9570 samples, validate on 7830 samples\n",
      "Epoch 1/10\n",
      "9570/9570 [==============================] - ETA: 1:36 - loss: 1.1016 - acc: 0.316 - ETA: 1:24 - loss: 1.1040 - acc: 0.318 - ETA: 1:18 - loss: 1.1026 - acc: 0.312 - ETA: 1:17 - loss: 1.1034 - acc: 0.312 - ETA: 1:13 - loss: 1.1033 - acc: 0.312 - ETA: 1:09 - loss: 1.1023 - acc: 0.316 - ETA: 1:06 - loss: 1.1016 - acc: 0.320 - ETA: 1:03 - loss: 1.1020 - acc: 0.318 - ETA: 1:01 - loss: 1.1019 - acc: 0.319 - ETA: 58s - loss: 1.1017 - acc: 0.319 - ETA: 56s - loss: 1.1013 - acc: 0.32 - ETA: 54s - loss: 1.1011 - acc: 0.32 - ETA: 52s - loss: 1.1010 - acc: 0.32 - ETA: 49s - loss: 1.1006 - acc: 0.33 - ETA: 47s - loss: 1.1002 - acc: 0.32 - ETA: 45s - loss: 1.1001 - acc: 0.33 - ETA: 43s - loss: 1.0999 - acc: 0.33 - ETA: 40s - loss: 1.0998 - acc: 0.33 - ETA: 39s - loss: 1.0997 - acc: 0.33 - ETA: 37s - loss: 1.0995 - acc: 0.33 - ETA: 34s - loss: 1.0991 - acc: 0.34 - ETA: 32s - loss: 1.0989 - acc: 0.34 - ETA: 30s - loss: 1.0987 - acc: 0.35 - ETA: 28s - loss: 1.0985 - acc: 0.35 - ETA: 26s - loss: 1.0982 - acc: 0.35 - ETA: 23s - loss: 1.0981 - acc: 0.35 - ETA: 21s - loss: 1.0979 - acc: 0.35 - ETA: 19s - loss: 1.0977 - acc: 0.35 - ETA: 17s - loss: 1.0975 - acc: 0.36 - ETA: 15s - loss: 1.0973 - acc: 0.36 - ETA: 13s - loss: 1.0970 - acc: 0.37 - ETA: 11s - loss: 1.0968 - acc: 0.37 - ETA: 9s - loss: 1.0965 - acc: 0.3796 - ETA: 7s - loss: 1.0962 - acc: 0.379 - ETA: 5s - loss: 1.0958 - acc: 0.379 - ETA: 2s - loss: 1.0958 - acc: 0.377 - ETA: 0s - loss: 1.0954 - acc: 0.378 - 114s 12ms/step - loss: 1.0953 - acc: 0.3775 - val_loss: 1.0855 - val_acc: 0.3295\n",
      "Epoch 2/10\n",
      "9570/9570 [==============================] - ETA: 1:14 - loss: 1.0854 - acc: 0.343 - ETA: 1:13 - loss: 1.0780 - acc: 0.386 - ETA: 1:13 - loss: 1.0782 - acc: 0.403 - ETA: 1:11 - loss: 1.0809 - acc: 0.425 - ETA: 1:08 - loss: 1.0812 - acc: 0.427 - ETA: 1:05 - loss: 1.0801 - acc: 0.453 - ETA: 1:03 - loss: 1.0794 - acc: 0.474 - ETA: 1:01 - loss: 1.0786 - acc: 0.484 - ETA: 58s - loss: 1.0779 - acc: 0.490 - ETA: 56s - loss: 1.0774 - acc: 0.49 - ETA: 55s - loss: 1.0766 - acc: 0.50 - ETA: 52s - loss: 1.0755 - acc: 0.51 - ETA: 50s - loss: 1.0743 - acc: 0.52 - ETA: 48s - loss: 1.0728 - acc: 0.54 - ETA: 46s - loss: 1.0719 - acc: 0.55 - ETA: 44s - loss: 1.0705 - acc: 0.55 - ETA: 42s - loss: 1.0697 - acc: 0.55 - ETA: 40s - loss: 1.0684 - acc: 0.55 - ETA: 38s - loss: 1.0672 - acc: 0.55 - ETA: 36s - loss: 1.0658 - acc: 0.56 - ETA: 34s - loss: 1.0644 - acc: 0.56 - ETA: 31s - loss: 1.0629 - acc: 0.57 - ETA: 29s - loss: 1.0614 - acc: 0.57 - ETA: 27s - loss: 1.0597 - acc: 0.57 - ETA: 25s - loss: 1.0580 - acc: 0.57 - ETA: 23s - loss: 1.0562 - acc: 0.57 - ETA: 21s - loss: 1.0549 - acc: 0.57 - ETA: 19s - loss: 1.0534 - acc: 0.57 - ETA: 17s - loss: 1.0530 - acc: 0.56 - ETA: 15s - loss: 1.0515 - acc: 0.55 - ETA: 13s - loss: 1.0495 - acc: 0.56 - ETA: 11s - loss: 1.0477 - acc: 0.56 - ETA: 9s - loss: 1.0464 - acc: 0.5629 - ETA: 7s - loss: 1.0445 - acc: 0.564 - ETA: 4s - loss: 1.0420 - acc: 0.573 - ETA: 2s - loss: 1.0393 - acc: 0.576 - ETA: 0s - loss: 1.0366 - acc: 0.577 - 113s 12ms/step - loss: 1.0359 - acc: 0.5778 - val_loss: 0.9430 - val_acc: 0.6701\n",
      "Epoch 3/10\n",
      "9570/9570 [==============================] - ETA: 1:14 - loss: 0.9315 - acc: 0.710 - ETA: 1:12 - loss: 0.9245 - acc: 0.773 - ETA: 1:13 - loss: 0.9217 - acc: 0.777 - ETA: 1:10 - loss: 0.9163 - acc: 0.756 - ETA: 1:07 - loss: 0.9164 - acc: 0.730 - ETA: 1:05 - loss: 0.9122 - acc: 0.746 - ETA: 1:02 - loss: 0.9045 - acc: 0.759 - ETA: 1:00 - loss: 0.8990 - acc: 0.752 - ETA: 58s - loss: 0.8981 - acc: 0.728 - ETA: 56s - loss: 0.8928 - acc: 0.73 - ETA: 55s - loss: 0.8881 - acc: 0.74 - ETA: 52s - loss: 0.8824 - acc: 0.75 - ETA: 50s - loss: 0.8769 - acc: 0.75 - ETA: 48s - loss: 0.8702 - acc: 0.76 - ETA: 46s - loss: 0.8642 - acc: 0.77 - ETA: 44s - loss: 0.8585 - acc: 0.77 - ETA: 42s - loss: 0.8531 - acc: 0.77 - ETA: 40s - loss: 0.8466 - acc: 0.78 - ETA: 38s - loss: 0.8394 - acc: 0.78 - ETA: 36s - loss: 0.8330 - acc: 0.79 - ETA: 34s - loss: 0.8251 - acc: 0.79 - ETA: 31s - loss: 0.8173 - acc: 0.80 - ETA: 29s - loss: 0.8106 - acc: 0.80 - ETA: 27s - loss: 0.8033 - acc: 0.81 - ETA: 25s - loss: 0.7944 - acc: 0.81 - ETA: 23s - loss: 0.7874 - acc: 0.81 - ETA: 21s - loss: 0.7793 - acc: 0.82 - ETA: 19s - loss: 0.7718 - acc: 0.82 - ETA: 17s - loss: 0.7649 - acc: 0.82 - ETA: 15s - loss: 0.7568 - acc: 0.83 - ETA: 13s - loss: 0.7497 - acc: 0.83 - ETA: 11s - loss: 0.7420 - acc: 0.83 - ETA: 9s - loss: 0.7337 - acc: 0.8366 - ETA: 7s - loss: 0.7251 - acc: 0.839 - ETA: 4s - loss: 0.7171 - acc: 0.842 - ETA: 2s - loss: 0.7093 - acc: 0.845 - ETA: 0s - loss: 0.7017 - acc: 0.847 - 113s 12ms/step - loss: 0.6985 - acc: 0.8483 - val_loss: 0.4183 - val_acc: 0.9114\n",
      "Epoch 4/10\n",
      "9570/9570 [==============================] - ETA: 1:16 - loss: 0.4040 - acc: 0.902 - ETA: 1:18 - loss: 0.3882 - acc: 0.912 - ETA: 1:15 - loss: 0.3888 - acc: 0.899 - ETA: 1:11 - loss: 0.3879 - acc: 0.899 - ETA: 1:08 - loss: 0.3829 - acc: 0.902 - ETA: 1:05 - loss: 0.3717 - acc: 0.913 - ETA: 1:03 - loss: 0.3655 - acc: 0.918 - ETA: 1:01 - loss: 0.3574 - acc: 0.920 - ETA: 59s - loss: 0.3530 - acc: 0.921 - ETA: 57s - loss: 0.3479 - acc: 0.92 - ETA: 55s - loss: 0.3411 - acc: 0.92 - ETA: 53s - loss: 0.3344 - acc: 0.92 - ETA: 50s - loss: 0.3322 - acc: 0.92 - ETA: 48s - loss: 0.3262 - acc: 0.93 - ETA: 46s - loss: 0.3221 - acc: 0.93 - ETA: 44s - loss: 0.3168 - acc: 0.93 - ETA: 42s - loss: 0.3135 - acc: 0.93 - ETA: 40s - loss: 0.3101 - acc: 0.93 - ETA: 38s - loss: 0.3042 - acc: 0.93 - ETA: 36s - loss: 0.3003 - acc: 0.93 - ETA: 34s - loss: 0.2952 - acc: 0.93 - ETA: 32s - loss: 0.2909 - acc: 0.94 - ETA: 29s - loss: 0.2898 - acc: 0.94 - ETA: 27s - loss: 0.2856 - acc: 0.94 - ETA: 25s - loss: 0.2825 - acc: 0.94 - ETA: 23s - loss: 0.2792 - acc: 0.94 - ETA: 21s - loss: 0.2766 - acc: 0.94 - ETA: 19s - loss: 0.2730 - acc: 0.94 - ETA: 17s - loss: 0.2693 - acc: 0.94 - ETA: 15s - loss: 0.2657 - acc: 0.94 - ETA: 13s - loss: 0.2628 - acc: 0.94 - ETA: 11s - loss: 0.2592 - acc: 0.94 - ETA: 9s - loss: 0.2562 - acc: 0.9484 - ETA: 7s - loss: 0.2531 - acc: 0.949 - ETA: 4s - loss: 0.2502 - acc: 0.949 - ETA: 2s - loss: 0.2477 - acc: 0.950 - ETA: 0s - loss: 0.2446 - acc: 0.951 - 113s 12ms/step - loss: 0.2438 - acc: 0.9514 - val_loss: 0.1322 - val_acc: 0.9798\n",
      "Epoch 5/10\n",
      "9570/9570 [==============================] - ETA: 1:14 - loss: 0.1140 - acc: 0.988 - ETA: 1:17 - loss: 0.1231 - acc: 0.988 - ETA: 1:13 - loss: 0.1227 - acc: 0.988 - ETA: 1:11 - loss: 0.1194 - acc: 0.988 - ETA: 1:08 - loss: 0.1216 - acc: 0.983 - ETA: 1:05 - loss: 0.1179 - acc: 0.985 - ETA: 1:03 - loss: 0.1174 - acc: 0.985 - ETA: 1:00 - loss: 0.1184 - acc: 0.981 - ETA: 59s - loss: 0.1210 - acc: 0.980 - ETA: 57s - loss: 0.1190 - acc: 0.98 - ETA: 55s - loss: 0.1194 - acc: 0.97 - ETA: 53s - loss: 0.1193 - acc: 0.97 - ETA: 50s - loss: 0.1190 - acc: 0.97 - ETA: 48s - loss: 0.1198 - acc: 0.97 - ETA: 46s - loss: 0.1197 - acc: 0.97 - ETA: 44s - loss: 0.1187 - acc: 0.97 - ETA: 42s - loss: 0.1177 - acc: 0.97 - ETA: 40s - loss: 0.1173 - acc: 0.97 - ETA: 38s - loss: 0.1171 - acc: 0.97 - ETA: 36s - loss: 0.1153 - acc: 0.97 - ETA: 34s - loss: 0.1140 - acc: 0.98 - ETA: 31s - loss: 0.1124 - acc: 0.98 - ETA: 29s - loss: 0.1112 - acc: 0.98 - ETA: 27s - loss: 0.1102 - acc: 0.98 - ETA: 25s - loss: 0.1099 - acc: 0.98 - ETA: 23s - loss: 0.1086 - acc: 0.98 - ETA: 21s - loss: 0.1080 - acc: 0.98 - ETA: 19s - loss: 0.1066 - acc: 0.98 - ETA: 17s - loss: 0.1059 - acc: 0.98 - ETA: 15s - loss: 0.1047 - acc: 0.98 - ETA: 13s - loss: 0.1034 - acc: 0.98 - ETA: 11s - loss: 0.1027 - acc: 0.98 - ETA: 9s - loss: 0.1019 - acc: 0.9832 - ETA: 7s - loss: 0.1007 - acc: 0.983 - ETA: 4s - loss: 0.1001 - acc: 0.983 - ETA: 2s - loss: 0.0991 - acc: 0.983 - ETA: 0s - loss: 0.0980 - acc: 0.984 - 113s 12ms/step - loss: 0.0979 - acc: 0.9840 - val_loss: 0.0684 - val_acc: 0.9897\n",
      "Epoch 6/10\n",
      "9570/9570 [==============================] - ETA: 1:21 - loss: 0.0640 - acc: 0.996 - ETA: 1:19 - loss: 0.0606 - acc: 0.996 - ETA: 1:15 - loss: 0.0564 - acc: 0.997 - ETA: 1:11 - loss: 0.0621 - acc: 0.996 - ETA: 1:08 - loss: 0.0612 - acc: 0.996 - ETA: 1:06 - loss: 0.0613 - acc: 0.994 - ETA: 1:03 - loss: 0.0608 - acc: 0.995 - ETA: 1:01 - loss: 0.0582 - acc: 0.995 - ETA: 1:00 - loss: 0.0582 - acc: 0.995 - ETA: 58s - loss: 0.0586 - acc: 0.994 - ETA: 55s - loss: 0.0575 - acc: 0.99 - ETA: 53s - loss: 0.0570 - acc: 0.99 - ETA: 51s - loss: 0.0563 - acc: 0.99 - ETA: 48s - loss: 0.0573 - acc: 0.99 - ETA: 46s - loss: 0.0566 - acc: 0.99 - ETA: 44s - loss: 0.0562 - acc: 0.99 - ETA: 42s - loss: 0.0564 - acc: 0.99 - ETA: 40s - loss: 0.0559 - acc: 0.99 - ETA: 38s - loss: 0.0551 - acc: 0.99 - ETA: 36s - loss: 0.0542 - acc: 0.99 - ETA: 34s - loss: 0.0541 - acc: 0.99 - ETA: 32s - loss: 0.0537 - acc: 0.99 - ETA: 29s - loss: 0.0531 - acc: 0.99 - ETA: 27s - loss: 0.0528 - acc: 0.99 - ETA: 25s - loss: 0.0530 - acc: 0.99 - ETA: 23s - loss: 0.0528 - acc: 0.99 - ETA: 21s - loss: 0.0527 - acc: 0.99 - ETA: 19s - loss: 0.0520 - acc: 0.99 - ETA: 17s - loss: 0.0517 - acc: 0.99 - ETA: 15s - loss: 0.0514 - acc: 0.99 - ETA: 13s - loss: 0.0508 - acc: 0.99 - ETA: 11s - loss: 0.0506 - acc: 0.99 - ETA: 9s - loss: 0.0508 - acc: 0.9953 - ETA: 7s - loss: 0.0504 - acc: 0.995 - ETA: 4s - loss: 0.0499 - acc: 0.995 - ETA: 2s - loss: 0.0498 - acc: 0.995 - ETA: 0s - loss: 0.0493 - acc: 0.995 - 113s 12ms/step - loss: 0.0493 - acc: 0.9956 - val_loss: 0.0407 - val_acc: 0.9944\n",
      "Epoch 7/10\n",
      "9570/9570 [==============================] - ETA: 1:22 - loss: 0.0337 - acc: 1.000 - ETA: 1:15 - loss: 0.0326 - acc: 1.000 - ETA: 1:11 - loss: 0.0344 - acc: 0.996 - ETA: 1:09 - loss: 0.0383 - acc: 0.992 - ETA: 1:06 - loss: 0.0376 - acc: 0.993 - ETA: 1:04 - loss: 0.0376 - acc: 0.993 - ETA: 1:02 - loss: 0.0368 - acc: 0.994 - ETA: 1:01 - loss: 0.0357 - acc: 0.994 - ETA: 59s - loss: 0.0358 - acc: 0.995 - ETA: 57s - loss: 0.0353 - acc: 0.99 - ETA: 54s - loss: 0.0354 - acc: 0.99 - ETA: 52s - loss: 0.0362 - acc: 0.99 - ETA: 50s - loss: 0.0362 - acc: 0.99 - ETA: 48s - loss: 0.0357 - acc: 0.99 - ETA: 46s - loss: 0.0351 - acc: 0.99 - ETA: 44s - loss: 0.0349 - acc: 0.99 - ETA: 42s - loss: 0.0346 - acc: 0.99 - ETA: 40s - loss: 0.0339 - acc: 0.99 - ETA: 38s - loss: 0.0336 - acc: 0.99 - ETA: 35s - loss: 0.0329 - acc: 0.99 - ETA: 33s - loss: 0.0327 - acc: 0.99 - ETA: 31s - loss: 0.0325 - acc: 0.99 - ETA: 29s - loss: 0.0324 - acc: 0.99 - ETA: 27s - loss: 0.0320 - acc: 0.99 - ETA: 25s - loss: 0.0318 - acc: 0.99 - ETA: 23s - loss: 0.0314 - acc: 0.99 - ETA: 21s - loss: 0.0313 - acc: 0.99 - ETA: 19s - loss: 0.0311 - acc: 0.99 - ETA: 17s - loss: 0.0310 - acc: 0.99 - ETA: 15s - loss: 0.0306 - acc: 0.99 - ETA: 13s - loss: 0.0304 - acc: 0.99 - ETA: 11s - loss: 0.0300 - acc: 0.99 - ETA: 9s - loss: 0.0300 - acc: 0.9966 - ETA: 6s - loss: 0.0299 - acc: 0.996 - ETA: 4s - loss: 0.0299 - acc: 0.996 - ETA: 2s - loss: 0.0297 - acc: 0.996 - ETA: 0s - loss: 0.0296 - acc: 0.996 - 113s 12ms/step - loss: 0.0296 - acc: 0.9966 - val_loss: 0.0302 - val_acc: 0.9955\n",
      "Epoch 8/10\n",
      "9570/9570 [==============================] - ETA: 1:17 - loss: 0.0230 - acc: 1.000 - ETA: 1:14 - loss: 0.0268 - acc: 0.996 - ETA: 1:11 - loss: 0.0237 - acc: 0.997 - ETA: 1:08 - loss: 0.0264 - acc: 0.996 - ETA: 1:06 - loss: 0.0256 - acc: 0.996 - ETA: 1:04 - loss: 0.0250 - acc: 0.996 - ETA: 1:02 - loss: 0.0256 - acc: 0.997 - ETA: 1:01 - loss: 0.0250 - acc: 0.997 - ETA: 58s - loss: 0.0244 - acc: 0.997 - ETA: 56s - loss: 0.0257 - acc: 0.99 - ETA: 54s - loss: 0.0255 - acc: 0.99 - ETA: 52s - loss: 0.0250 - acc: 0.99 - ETA: 50s - loss: 0.0248 - acc: 0.99 - ETA: 48s - loss: 0.0241 - acc: 0.99 - ETA: 46s - loss: 0.0235 - acc: 0.99 - ETA: 44s - loss: 0.0236 - acc: 0.99 - ETA: 42s - loss: 0.0235 - acc: 0.99 - ETA: 40s - loss: 0.0232 - acc: 0.99 - ETA: 38s - loss: 0.0233 - acc: 0.99 - ETA: 35s - loss: 0.0232 - acc: 0.99 - ETA: 33s - loss: 0.0230 - acc: 0.99 - ETA: 31s - loss: 0.0226 - acc: 0.99 - ETA: 29s - loss: 0.0224 - acc: 0.99 - ETA: 27s - loss: 0.0221 - acc: 0.99 - ETA: 25s - loss: 0.0219 - acc: 0.99 - ETA: 23s - loss: 0.0221 - acc: 0.99 - ETA: 21s - loss: 0.0220 - acc: 0.99 - ETA: 19s - loss: 0.0219 - acc: 0.99 - ETA: 17s - loss: 0.0219 - acc: 0.99 - ETA: 15s - loss: 0.0218 - acc: 0.99 - ETA: 13s - loss: 0.0217 - acc: 0.99 - ETA: 11s - loss: 0.0216 - acc: 0.99 - ETA: 9s - loss: 0.0215 - acc: 0.9981 - ETA: 7s - loss: 0.0216 - acc: 0.997 - ETA: 4s - loss: 0.0215 - acc: 0.998 - ETA: 2s - loss: 0.0213 - acc: 0.998 - ETA: 0s - loss: 0.0214 - acc: 0.998 - 114s 12ms/step - loss: 0.0214 - acc: 0.9980 - val_loss: 0.0202 - val_acc: 0.9982\n",
      "Epoch 9/10\n",
      "9570/9570 [==============================] - ETA: 1:15 - loss: 0.0138 - acc: 1.000 - ETA: 1:13 - loss: 0.0136 - acc: 1.000 - ETA: 1:10 - loss: 0.0184 - acc: 0.998 - ETA: 1:08 - loss: 0.0184 - acc: 0.999 - ETA: 1:07 - loss: 0.0181 - acc: 0.999 - ETA: 1:07 - loss: 0.0172 - acc: 0.999 - ETA: 1:06 - loss: 0.0172 - acc: 0.999 - ETA: 1:03 - loss: 0.0171 - acc: 0.999 - ETA: 1:00 - loss: 0.0170 - acc: 0.999 - ETA: 58s - loss: 0.0179 - acc: 0.999 - ETA: 55s - loss: 0.0179 - acc: 0.99 - ETA: 53s - loss: 0.0178 - acc: 0.99 - ETA: 51s - loss: 0.0174 - acc: 0.99 - ETA: 49s - loss: 0.0175 - acc: 0.99 - ETA: 47s - loss: 0.0170 - acc: 0.99 - ETA: 45s - loss: 0.0169 - acc: 0.99 - ETA: 42s - loss: 0.0181 - acc: 0.99 - ETA: 40s - loss: 0.0177 - acc: 0.99 - ETA: 38s - loss: 0.0175 - acc: 0.99 - ETA: 36s - loss: 0.0174 - acc: 0.99 - ETA: 34s - loss: 0.0179 - acc: 0.99 - ETA: 32s - loss: 0.0178 - acc: 0.99 - ETA: 30s - loss: 0.0177 - acc: 0.99 - ETA: 28s - loss: 0.0179 - acc: 0.99 - ETA: 25s - loss: 0.0177 - acc: 0.99 - ETA: 23s - loss: 0.0176 - acc: 0.99 - ETA: 21s - loss: 0.0175 - acc: 0.99 - ETA: 19s - loss: 0.0176 - acc: 0.99 - ETA: 17s - loss: 0.0176 - acc: 0.99 - ETA: 15s - loss: 0.0173 - acc: 0.99 - ETA: 13s - loss: 0.0171 - acc: 0.99 - ETA: 11s - loss: 0.0171 - acc: 0.99 - ETA: 9s - loss: 0.0172 - acc: 0.9989 - ETA: 7s - loss: 0.0171 - acc: 0.999 - ETA: 4s - loss: 0.0171 - acc: 0.998 - ETA: 2s - loss: 0.0172 - acc: 0.998 - ETA: 0s - loss: 0.0170 - acc: 0.998 - 114s 12ms/step - loss: 0.0170 - acc: 0.9987 - val_loss: 0.0174 - val_acc: 0.9978\n",
      "Epoch 10/10\n",
      "9570/9570 [==============================] - ETA: 1:16 - loss: 0.0191 - acc: 0.996 - ETA: 1:14 - loss: 0.0158 - acc: 0.998 - ETA: 1:10 - loss: 0.0153 - acc: 0.998 - ETA: 1:08 - loss: 0.0151 - acc: 0.999 - ETA: 1:06 - loss: 0.0139 - acc: 0.999 - ETA: 1:05 - loss: 0.0138 - acc: 0.999 - ETA: 1:03 - loss: 0.0135 - acc: 0.999 - ETA: 1:01 - loss: 0.0130 - acc: 0.999 - ETA: 58s - loss: 0.0128 - acc: 0.999 - ETA: 56s - loss: 0.0129 - acc: 0.99 - ETA: 54s - loss: 0.0126 - acc: 0.99 - ETA: 52s - loss: 0.0126 - acc: 0.99 - ETA: 50s - loss: 0.0128 - acc: 0.99 - ETA: 48s - loss: 0.0128 - acc: 0.99 - ETA: 46s - loss: 0.0125 - acc: 0.99 - ETA: 44s - loss: 0.0127 - acc: 0.99 - ETA: 42s - loss: 0.0124 - acc: 0.99 - ETA: 40s - loss: 0.0122 - acc: 0.99 - ETA: 38s - loss: 0.0124 - acc: 0.99 - ETA: 35s - loss: 0.0122 - acc: 0.99 - ETA: 34s - loss: 0.0122 - acc: 0.99 - ETA: 32s - loss: 0.0124 - acc: 0.99 - ETA: 29s - loss: 0.0121 - acc: 0.99 - ETA: 27s - loss: 0.0121 - acc: 0.99 - ETA: 25s - loss: 0.0121 - acc: 0.99 - ETA: 23s - loss: 0.0120 - acc: 0.99 - ETA: 21s - loss: 0.0119 - acc: 0.99 - ETA: 19s - loss: 0.0121 - acc: 0.99 - ETA: 17s - loss: 0.0119 - acc: 0.99 - ETA: 15s - loss: 0.0119 - acc: 0.99 - ETA: 13s - loss: 0.0119 - acc: 0.99 - ETA: 11s - loss: 0.0118 - acc: 0.99 - ETA: 9s - loss: 0.0118 - acc: 0.9994 - ETA: 7s - loss: 0.0116 - acc: 0.999 - ETA: 4s - loss: 0.0116 - acc: 0.999 - ETA: 2s - loss: 0.0115 - acc: 0.999 - ETA: 0s - loss: 0.0114 - acc: 0.999 - 113s 12ms/step - loss: 0.0119 - acc: 0.9994 - val_loss: 0.0190 - val_acc: 0.9958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"print(history.history.keys())\\n# summarize history for accuracy\\nplt.plot(history.history['acc'])\\nplt.plot(history.history['val_acc'])\\nplt.title('model accuracy')\\nplt.ylabel('accuracy')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'test'], loc='upper left')\\nplt.show()\\n# summarize history for loss\\nplt.plot(history.history['loss'])\\nplt.plot(history.history['val_loss'])\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'test'], loc='upper left')\\nplt.show()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,batch_size=batch_size,epochs=n_epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "# list all data in history\n",
    "'''print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "\n",
    "#for i in range(1):\n",
    "    #return_value, image = camera.read()\n",
    "    #cv2.imwrite('opencv'+str(i)+'.jpg', image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Middle\n",
      "[[0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:/Users/Harsh Pande/Desktop/Data Science/nerur.jpg', target_size = (50, 50))\n",
    "#test_image = test_image.convert(\"L\"\n",
    "test_image = image.img_to_array(test_image)\n",
    "#print(test_image.shape)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "\n",
    "\n",
    "if (result[0][0] > result[0][1]) and (result[0][0] > result[0][2]):\n",
    "    print(\"Infant\")\n",
    "if (result[0][1] > result[0][0]) and (result[0][1] > result[0][2]):\n",
    "    print(\"Middle\")\n",
    "elif (result[0][2] > result[0][0]) and (result[0][2] > result[0][1]):\n",
    "    print(\"Old\")\n",
    "        \n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
